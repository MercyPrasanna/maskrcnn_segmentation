{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '/subscriptions/601f4351-33bb-4d76-96ca-886940409b3d/resourceGroups/mlopcent-AML-RG/providers/Microsoft.MachineLearningServices/workspaces/mlopcent-AML-WS',\n",
       " 'name': 'mlopcent-AML-WS',\n",
       " 'location': 'centralus',\n",
       " 'type': 'Microsoft.MachineLearningServices/workspaces',\n",
       " 'sku': 'Enterprise',\n",
       " 'workspaceid': '9f71fb47-35b4-4cde-9787-6879d312a8e1',\n",
       " 'description': '',\n",
       " 'friendlyName': 'mlopcent-AML-WS',\n",
       " 'creationTime': '2019-11-12T01:09:07.4987878+00:00',\n",
       " 'containerRegistry': '/subscriptions/601f4351-33bb-4d76-96ca-886940409b3d/resourcegroups/mlopcent-aml-rg/providers/microsoft.containerregistry/registries/mlopcentamlcr',\n",
       " 'keyVault': '/subscriptions/601f4351-33bb-4d76-96ca-886940409b3d/resourcegroups/mlopcent-aml-rg/providers/microsoft.keyvault/vaults/mlopcent-aml-kv',\n",
       " 'applicationInsights': '/subscriptions/601f4351-33bb-4d76-96ca-886940409b3d/resourcegroups/mlopcent-aml-rg/providers/microsoft.insights/components/mlopcent-aml-ai',\n",
       " 'identityPrincipalId': '159e1229-ee0a-44ad-9432-149487018400',\n",
       " 'identityTenantId': '72f988bf-86f1-41af-91ab-2d7cd011db47',\n",
       " 'identityType': 'SystemAssigned',\n",
       " 'storageAccount': '/subscriptions/601f4351-33bb-4d76-96ca-886940409b3d/resourcegroups/mlopcent-aml-rg/providers/microsoft.storage/storageaccounts/mlopcentamlsa'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "subscription_id = os.getenv(\"SUBSCRIPTION_ID\", default=\"601f4351-33bb-4d76-96ca-886940409b3d\")\n",
    "resource_group = os.getenv(\"RESOURCE_GROUP\", default=\"mlopcent-AML-RG\")\n",
    "workspace_name = os.getenv(\"WORKSPACE_NAME\", default=\"mlopcent-AML-WS\")\n",
    "workspace_region = os.getenv(\"WORKSPACE_REGION\", default=\"centralus\")\n",
    "\n",
    "from azureml.core import Workspace\n",
    "\n",
    "# Create the workspace using the specified parameters\n",
    "ws = Workspace.create(name = workspace_name,\n",
    "                      subscription_id = subscription_id,\n",
    "                      resource_group = resource_group, \n",
    "                      location = workspace_region,\n",
    "                      create_resource_group = False,\n",
    "                      sku = 'basic',\n",
    "                      exist_ok = True)\n",
    "ws.get_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Datastore\n",
    "blob_datastore_name='isic2018' # Name of the Datastore  to workspace\n",
    "container_name=os.getenv(\"BLOB_CONTAINER\", \"isic2018\") # Name of Azure blob container\n",
    "account_name=os.getenv(\"BLOB_ACCOUNTNAME\", \"mlblobdatastore\") # Storage account name\n",
    "account_key=os.getenv(\"BLOB_ACCOUNT_KEY\", \"bPlInBOqf0kfPpSNYeemRKNiOfcWsMWAUfR3ieyTUpxBKn/FEkZG9RgHUQfVjNtI3ky32wZ+LrjCe/oVC9M2eg==\") # Storage account key\n",
    "\n",
    "blob_datastore = Datastore.register_azure_blob_container(workspace=ws, \n",
    "                                                         datastore_name=blob_datastore_name, \n",
    "                                                         container_name=container_name, \n",
    "                                                         account_name=account_name,\n",
    "                                                         account_key=account_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Datastore\n",
    "#get named datastore from current workspace\n",
    "datastore = Datastore.get(ws, datastore_name='isic2018')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "output_dir = PipelineData(name=\"inferences_singlenode\", \n",
    "                          datastore=datastore, \n",
    "                          output_path_on_compute=\"ISIC2012_MaskRCNN_SingleNode_Inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found compute target. just use it. cpucluster\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "import os\n",
    "\n",
    "# choose a name for your cluster\n",
    "compute_name = os.environ.get('AML_COMPUTE_CLUSTER_NAME', 'cpucluster')\n",
    "compute_min_nodes = os.environ.get('AML_COMPUTE_CLUSTER_MIN_NODES', 0)\n",
    "compute_max_nodes = os.environ.get('AML_COMPUTE_CLUSTER_MAX_NODES', 4)\n",
    "\n",
    "# This example uses CPU VM. For using GPU VM, set SKU to STANDARD_NC6\n",
    "vm_size = os.environ.get('AML_COMPUTE_CLUSTER_SKU', 'STANDARD_D2_V2')\n",
    "\n",
    "\n",
    "if compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print('found compute target. just use it. ' + compute_name)\n",
    "else:\n",
    "    print('creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size=vm_size,\n",
    "                                                                min_nodes=compute_min_nodes, \n",
    "                                                                max_nodes=compute_max_nodes)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
    "    \n",
    "    # can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # if no min node count is provided it will use the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    \n",
    "     # For a more detailed view of current AmlCompute status, use get_status()\n",
    "    print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# import the necessary packages\n",
      "from mrcnn.config import Config\n",
      "from mrcnn import model as modellib\n",
      "from mrcnn import utils\n",
      "import numpy as np\n",
      "import imutils\n",
      "import cv2\n",
      "import os\n",
      "from azureml.core import Model\n",
      "import argparse\n",
      "import shutil\n",
      "\n",
      "\n",
      "# initialize the class names dictionary\n",
      "CLASS_NAMES = {1: \"lesion\"}\n",
      "\n",
      "class LesionBoundaryInferenceConfig(Config):\n",
      "\t# set the number of GPUs and images per GPU (which may be\n",
      "\t# different values than the ones used for training)\n",
      "\n",
      "\tNAME = \"lesion\"\n",
      "\tNUM_CLASSES = len(CLASS_NAMES) + 1\n",
      "\tGPU_COUNT = 1\n",
      "\tIMAGES_PER_GPU = 1\n",
      "\n",
      "\t# set the minimum detection confidence (used to prune out false\n",
      "\t# positive detections)\n",
      "\tDETECTION_MIN_CONFIDENCE = 0.9\n",
      "\n",
      "\n",
      "\n",
      "# construct the argument parser and parse the arguments\n",
      "ap = argparse.ArgumentParser()\n",
      "ap.add_argument('--output_dir', type=str, dest='output_dir', help='output dir')\n",
      "ap.add_argument('--dataset_path', dest=\"dataset_path\", required=True)\n",
      "args = vars(ap.parse_args())\n",
      "\n",
      "\n",
      "\n",
      "# create output directory if it does not exist\n",
      "\n",
      "output_dir = args[\"output_dir\"]\n",
      "\n",
      "print(\"###############output_dir\",output_dir)\n",
      "\n",
      "os.makedirs(output_dir, exist_ok=True)\n",
      "\n",
      "data_dir = args[\"dataset_path\"]\n",
      "\n",
      "input_dir = data_dir + '/' + 'ISIC2018_Task1-2_Training_Input'\n",
      "\n",
      "groundtruth_dir = data_dir + '/' + 'ISIC2018_Task1_Training_GroundTruth'\n",
      "\n",
      "file_paths = []\n",
      "\n",
      "image_list = os.listdir(input_dir)\n",
      "\n",
      "\n",
      "file_paths = [input_dir + '/' + file_name.rstrip() for file_name in image_list]\n",
      "\n",
      "LOGS_AND_MODEL_DIR = \"./outputs/lesions_logs\"\n",
      "os.makedirs(LOGS_AND_MODEL_DIR, exist_ok=True)\n",
      "\n",
      "model_path = Model.get_model_path(\"mask_rcnn_horovod\")\n",
      "\n",
      "# initialize the inference configuration\n",
      "config = LesionBoundaryInferenceConfig()\n",
      "\n",
      "# initialize the Mask R-CNN model for inference\n",
      "model = modellib.MaskRCNN(mode=\"inference\", config=config,model_dir=LOGS_AND_MODEL_DIR)\n",
      "\n",
      "\n",
      "model.load_weights('mask_rcnn_lesion_0017_singlenode.h5', by_name=True)\n",
      "\n",
      "for image_file in file_paths:\n",
      "\n",
      "\tout_filename = os.path.join(output_dir, os.path.basename(image_file))\n",
      "\n",
      "\tfilename = os.path.basename(image_file).split(\".\")[0]\n",
      "\n",
      "\tsegmentation_file = os.path.sep.join([groundtruth_dir,\n",
      "\t\t\t\"{}_segmentation.png\".format(filename)])\n",
      "\t\n",
      "\tout_segmentation_filename = os.path.join(output_dir, os.path.basename(segmentation_file))\n",
      "\n",
      "\tprint(\"filename##################################\",filename)\n",
      "\tprint(\"outfilename##################################\",out_filename)\n",
      "\tprint(\"segmentationfilename##################################\",segmentation_file)\n",
      "\n",
      "\n",
      "\t# load the input image, convert it from BGR to RGB channel\n",
      "\t# ordering, and resize the image\n",
      "\timage = cv2.imread(image_file)\n",
      "\timage, window, scale, padding, crop = utils.resize_image(image, min_dim=config.IMAGE_MIN_DIM, min_scale=config.IMAGE_MIN_SCALE, max_dim=config.IMAGE_MAX_DIM, mode=config.IMAGE_RESIZE_MODE)\n",
      "    \n",
      "\tgroundtruth_image = cv2.imread(segmentation_file)\n",
      "\tgroundtruth_mask = utils.resize_mask(groundtruth_image, scale, padding, crop)\n",
      "\n",
      "\t#image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "\t#image = imutils.resize(image, width=1024)\n",
      "\n",
      "\timage_mask = np.zeros(image.shape)\n",
      "\n",
      "\tprint(\"#################input image shape\",image.shape)\n",
      "\tprint(\"#################ground truth mask shape\",groundtruth_mask.shape)\n",
      "\tprint(\"#################predicted mask shape\",image_mask.shape)\n",
      "\n",
      "\t# perform a forward pass of the network to obtain the results\n",
      "\tr = model.detect([image], verbose=1)[0]\n",
      "\n",
      "\t# loop over of the detected object's bounding boxes and\n",
      "\t# masks, drawing each as we go along\n",
      "\tfor i in range(0, r[\"rois\"].shape[0]):\n",
      "\t\tmask = r[\"masks\"][:, :, i]\n",
      "\t\tfor c in range(3):\n",
      "\t\t\timage_mask[:, :, c] = np.where(mask == 1,255,0)\n",
      "\t\t\tcv2.imwrite(out_filename, image_mask)\n",
      "\t\t\tcv2.imwrite(out_segmentation_filename, groundtruth_mask)\n",
      "\t\t\tshutil.copy(out_filename, \"./outputs/\")\n",
      "\t\t\tshutil.copy(out_segmentation_filename, \"./outputs/\")\n",
      "\t\t\tprint(\"copied file to the outputs\")\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "scripts_folder = \"mask_rcnn_singlenode_inference\"\n",
    "script_file = \"lesions_inference.py\"\n",
    "\n",
    "# peek at contents\n",
    "with open(os.path.join(scripts_folder, script_file)) as inference_file:\n",
    "    print(inference_file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.data.data_reference import DataReference\n",
    "input_images = DataReference(datastore=datastore, \n",
    "                             data_reference_name=\"input_images\",\n",
    "                             mode=\"mount\"\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.environment import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# set up environment\n",
    "env = Environment('isic_env')\n",
    "cd = CondaDependencies.create(pip_packages=['IPython[all]','scikit-image','cython','Pillow','numpy','scipy','azureml-sdk','tensorflow','keras','matplotlib','azureml-dataprep[pandas,fuse]>=1.1.14','imgaug','imutils','opencv-python','h5py'])\n",
    "\n",
    "env.python.conda_dependencies = cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - This compute target type doesn't support non-Docker runs; overriding run configuration enable Docker.\n"
     ]
    }
   ],
   "source": [
    "from azureml.train.estimator import Estimator\n",
    "\n",
    "est = Estimator(source_directory=scripts_folder, \n",
    "                compute_target=compute_target, \n",
    "                entry_script=script_file, \n",
    "                environment_definition= env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.steps import EstimatorStep\n",
    "\n",
    "est_step = EstimatorStep(name=\"Estimator_Train\", \n",
    "                         estimator=est, \n",
    "                         estimator_entry_script_arguments=[\"--output_dir\", output_dir, \"--dataset_path\", input_images],\n",
    "                         runconfig_pipeline_params=None, \n",
    "                         inputs=[input_images], \n",
    "                         outputs=[output_dir], \n",
    "                         compute_target=compute_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.core.experiment import Experiment\n",
    "\n",
    "pipeline = Pipeline(workspace=ws, steps=[est_step])\n",
    "pipeline_run = Experiment(ws, 'infer_pipeline_maskrcnn_singlenode').submit(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run.cancel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f59b847ef8d84854af02426db2e12312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_PipelineWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(pipeline_run).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run.wait_for_completion(show_output=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
